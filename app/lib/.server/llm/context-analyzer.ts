import type { Message } from 'ai';
import { createScopedLogger } from '~/utils/logger';
import {
  countMessagesTokens,
  countSystemTokens,
  getModelContextWindow,
  countMessageTokens,
} from './token-counter';

const logger = createScopedLogger('context-analyzer');

export interface ContextAnalysis {
  totalTokens: number;
  systemTokens: number;
  messageTokens: number;
  contextFilesTokens: number;
  availableTokens: number;
  utilizationPercentage: number;
  isNearLimit: boolean;
  isOverLimit: boolean;
  recommendations: string[];
  messageBreakdown: MessageTokenBreakdown[];
}

export interface MessageTokenBreakdown {
  messageId: string;
  role: 'user' | 'assistant' | 'system';
  tokens: number;
  contentPreview: string;
  isLarge: boolean;
}

/**
 * Analyzes context usage and provides recommendations
 */
export class ContextAnalyzer {
  private model: string;
  private maxContextTokens: number;
  private completionTokens: number;

  constructor(model: string, completionTokens: number = 4000) {
    this.model = model;
    this.maxContextTokens = getModelContextWindow(model);
    this.completionTokens = completionTokens;
  }

  /**
   * Analyze the current context usage
   */
  analyzeContext(
    messages: Message[],
    systemPrompt: string,
    contextFiles?: string
  ): ContextAnalysis {
    const systemTokens = countSystemTokens(systemPrompt, undefined, this.model);
    const contextFilesTokens = contextFiles ? countSystemTokens('', contextFiles, this.model) : 0;
    const messageTokens = countMessagesTokens(messages, this.model);
    const totalTokens = systemTokens + contextFilesTokens + messageTokens;
    
    const availableTokens = Math.max(0, this.maxContextTokens - totalTokens - this.completionTokens);
    const utilizationPercentage = (totalTokens / this.maxContextTokens) * 100;
    
    const isNearLimit = utilizationPercentage > 80;
    const isOverLimit = totalTokens + this.completionTokens > this.maxContextTokens;

    const messageBreakdown = this.analyzeMessages(messages);
    const recommendations = this.generateRecommendations(
      totalTokens,
      messageTokens,
      contextFilesTokens,
      messageBreakdown,
      isNearLimit,
      isOverLimit
    );

    return {
      totalTokens,
      systemTokens,
      messageTokens,
      contextFilesTokens,
      availableTokens,
      utilizationPercentage,
      isNearLimit,
      isOverLimit,
      recommendations,
      messageBreakdown,
    };
  }

  /**
   * Analyze individual messages
   */
  private analyzeMessages(messages: Message[]): MessageTokenBreakdown[] {
    return messages.map((message) => {
      const tokens = countMessageTokens(message, this.model);
      const contentPreview = this.getContentPreview(message.content);
      const isLarge = tokens > 500; // Consider messages over 500 tokens as large

      return {
        messageId: message.id || 'unknown',
        role: message.role,
        tokens,
        contentPreview,
        isLarge,
      };
    });
  }

  /**
   * Get a preview of message content
   */
  private getContentPreview(content: string | any[]): string {
    if (typeof content === 'string') {
      return content.length > 100 ? content.substring(0, 100) + '...' : content;
    } else if (Array.isArray(content)) {
      const textParts = content.filter(part => part.type === 'text').map(part => part.text);
      const preview = textParts.join(' ');
      return preview.length > 100 ? preview.substring(0, 100) + '...' : preview;
    }
    return '[Non-text content]';
  }

  /**
   * Generate recommendations based on analysis
   */
  private generateRecommendations(
    totalTokens: number,
    messageTokens: number,
    contextFilesTokens: number,
    messageBreakdown: MessageTokenBreakdown[],
    isNearLimit: boolean,
    isOverLimit: boolean
  ): string[] {
    const recommendations: string[] = [];

    if (isOverLimit) {
      recommendations.push('ðŸš¨ Context limit exceeded! The conversation will fail unless optimized.');
      recommendations.push('ðŸ’¡ Start a new conversation to reset the context.');
      recommendations.push('âš™ï¸ Enable context optimization in settings to automatically manage long conversations.');
    } else if (isNearLimit) {
      recommendations.push('âš ï¸ Approaching context limit. Consider optimizing soon.');
    }

    // Analyze message distribution
    const largeMessages = messageBreakdown.filter(m => m.isLarge);
    if (largeMessages.length > 0) {
      recommendations.push(`ðŸ“ ${largeMessages.length} large messages detected. Consider breaking them into smaller parts.`);
    }

    // Context files recommendations
    if (contextFilesTokens > messageTokens) {
      recommendations.push('ðŸ“ Context files are using more tokens than messages. Consider reducing file context.');
    }

    // Conversation length recommendations
    if (messageBreakdown.length > 20) {
      recommendations.push('ðŸ’¬ Long conversation detected. Consider summarizing or starting fresh.');
    }

    // Model-specific recommendations
    if (this.maxContextTokens < 32000) {
      recommendations.push('ðŸ”„ Consider using a model with a larger context window for long conversations.');
    }

    // General optimization tips
    if (isNearLimit || isOverLimit) {
      recommendations.push('âœ‚ï¸ Remove unnecessary messages or files from context.');
      recommendations.push('ðŸ“‹ Use summarization to condense conversation history.');
      recommendations.push('ðŸŽ¯ Focus on the most recent and relevant messages.');
    }

    return recommendations;
  }

  /**
   * Get context usage summary as a formatted string
   */
  getUsageSummary(analysis: ContextAnalysis): string {
    const { totalTokens, availableTokens, utilizationPercentage, isOverLimit, isNearLimit } = analysis;
    
    let status = 'âœ… Normal';
    if (isOverLimit) {
      status = 'ðŸš¨ Over Limit';
    } else if (isNearLimit) {
      status = 'âš ï¸ Near Limit';
    }

    return `Context Usage: ${totalTokens.toLocaleString()} / ${this.maxContextTokens.toLocaleString()} tokens (${utilizationPercentage.toFixed(1)}%)
Available: ${availableTokens.toLocaleString()} tokens
Status: ${status}

Breakdown:
- System: ${analysis.systemTokens.toLocaleString()} tokens
- Messages: ${analysis.messageTokens.toLocaleString()} tokens
- Context Files: ${analysis.contextFilesTokens.toLocaleString()} tokens
- Reserved for Completion: ${this.completionTokens.toLocaleString()} tokens`;
  }

  /**
   * Get optimization suggestions based on current usage
   */
  getOptimizationSuggestions(analysis: ContextAnalysis): string[] {
    const suggestions: string[] = [];

    if (analysis.isOverLimit || analysis.isNearLimit) {
      // Prioritize suggestions based on what's using the most tokens
      const tokenSources = [
        { name: 'Messages', tokens: analysis.messageTokens },
        { name: 'Context Files', tokens: analysis.contextFilesTokens },
        { name: 'System Prompt', tokens: analysis.systemTokens },
      ].sort((a, b) => b.tokens - a.tokens);

      suggestions.push(`Primary optimization target: ${tokenSources[0].name} (${tokenSources[0].tokens.toLocaleString()} tokens)`);

      if (tokenSources[0].name === 'Messages') {
        suggestions.push('Consider enabling automatic message truncation');
        suggestions.push('Remove older messages that are no longer relevant');
        suggestions.push('Summarize long conversation threads');
      } else if (tokenSources[0].name === 'Context Files') {
        suggestions.push('Reduce the number of files in context');
        suggestions.push('Focus on only the most relevant files');
        suggestions.push('Use file summaries instead of full content');
      }
    }

    return suggestions;
  }
}
